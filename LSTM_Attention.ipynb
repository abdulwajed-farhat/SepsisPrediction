{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient_Id</th>\n",
       "      <th>HR</th>\n",
       "      <th>O2Sat</th>\n",
       "      <th>Temp</th>\n",
       "      <th>SBP</th>\n",
       "      <th>MAP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>Resp</th>\n",
       "      <th>EtCO2</th>\n",
       "      <th>BaseExcess</th>\n",
       "      <th>...</th>\n",
       "      <th>sig_30</th>\n",
       "      <th>sig_31</th>\n",
       "      <th>sig_32</th>\n",
       "      <th>sig_33</th>\n",
       "      <th>sig_34</th>\n",
       "      <th>sig_35</th>\n",
       "      <th>sig_36</th>\n",
       "      <th>sig_37</th>\n",
       "      <th>sig_38</th>\n",
       "      <th>SepsisLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p000001</td>\n",
       "      <td>97.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.11</td>\n",
       "      <td>98.0</td>\n",
       "      <td>75.33</td>\n",
       "      <td>63.995</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p000001</td>\n",
       "      <td>97.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.11</td>\n",
       "      <td>98.0</td>\n",
       "      <td>75.33</td>\n",
       "      <td>63.995</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p000001</td>\n",
       "      <td>89.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>36.11</td>\n",
       "      <td>122.0</td>\n",
       "      <td>86.00</td>\n",
       "      <td>68.000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p000001</td>\n",
       "      <td>90.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.11</td>\n",
       "      <td>122.0</td>\n",
       "      <td>86.00</td>\n",
       "      <td>68.000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p000001</td>\n",
       "      <td>103.0</td>\n",
       "      <td>88.5</td>\n",
       "      <td>36.11</td>\n",
       "      <td>122.0</td>\n",
       "      <td>91.33</td>\n",
       "      <td>75.995</td>\n",
       "      <td>24.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Patient_Id     HR  O2Sat   Temp    SBP    MAP     DBP  Resp  EtCO2  \\\n",
       "0    p000001   97.0   95.0  36.11   98.0  75.33  63.995  19.0    NaN   \n",
       "1    p000001   97.0   95.0  36.11   98.0  75.33  63.995  19.0    NaN   \n",
       "2    p000001   89.0   99.0  36.11  122.0  86.00  68.000  22.0    NaN   \n",
       "3    p000001   90.0   95.0  36.11  122.0  86.00  68.000  30.0    NaN   \n",
       "4    p000001  103.0   88.5  36.11  122.0  91.33  75.995  24.5    NaN   \n",
       "\n",
       "   BaseExcess  ...  sig_30  sig_31  sig_32  sig_33  sig_34  sig_35  sig_36  \\\n",
       "0        24.0  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "1        24.0  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2        24.0  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "3        24.0  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "4        24.0  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "   sig_37  sig_38  SepsisLabel  \n",
       "0     NaN     NaN            0  \n",
       "1     NaN     NaN            0  \n",
       "2     NaN     NaN            0  \n",
       "3     NaN     NaN            0  \n",
       "4     NaN     NaN            0  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Users/farhat/Documents/Project/ProcessedData/fullData.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 40336 is out of bounds for axis 0 with size 40336",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m----> 5\u001b[0m     new_data\u001b[38;5;241m.\u001b[39mappend(data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPatient_Id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mid_unique[i]]\u001b[38;5;241m.\u001b[39miloc[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      6\u001b[0m     new_labels\u001b[38;5;241m.\u001b[39mappend(data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPatient_Id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mid_unique[i]]\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[0;31mIndexError\u001b[0m: index 40336 is out of bounds for axis 0 with size 40336"
     ]
    }
   ],
   "source": [
    "id_unique = data['Patient_Id'].unique()\n",
    "new_data = []\n",
    "new_labels = []\n",
    "for i in range(data.shape[0]-1):\n",
    "    new_data.append(data[data['Patient_Id']==id_unique[i]].iloc[:,:-1].reset_index(drop=True))\n",
    "    new_labels.append(data[data['Patient_Id']==id_unique[i]].iloc[:,-1].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 83)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[40335].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zc/cx2984q17xs9k62vy4ms148w0000gn/T/ipykernel_14293/68976783.py:313: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(ID_COL, group_keys=False).apply(lambda g: per_patient_impute(g, feature_cols))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using POS_WEIGHT=53.581\n",
      "Training…\n",
      "Epoch 01 | train loss 1.3129 auroc 0.6981 auprc 0.0380 f1 0.0738 | val loss 1.2645 auroc 0.7452 auprc 0.0533 f1 0.0929\n",
      "Epoch 02 | train loss 1.0806 auroc 0.8390 auprc 0.0800 f1 0.1056 | val loss 0.8634 auroc 0.9224 auprc 0.2280 f1 0.1491\n",
      "Epoch 03 | train loss 0.8294 auroc 0.9142 auprc 0.1373 f1 0.1681 | val loss 0.7432 auroc 0.9294 auprc 0.1925 f1 0.1527\n",
      "Epoch 04 | train loss 0.7247 auroc 0.9341 auprc 0.1796 f1 0.2005 | val loss 0.6251 auroc 0.9501 auprc 0.2742 f1 0.2449\n",
      "Epoch 05 | train loss 0.5902 auroc 0.9532 auprc 0.2461 f1 0.2315 | val loss 0.5430 auroc 0.9624 auprc 0.3624 f1 0.2556\n",
      "Epoch 06 | train loss 0.5407 auroc 0.9599 auprc 0.2918 f1 0.2589 | val loss 0.4993 auroc 0.9641 auprc 0.3470 f1 0.2447\n",
      "Epoch 07 | train loss 0.4743 auroc 0.9684 auprc 0.3552 f1 0.2860 | val loss 0.6452 auroc 0.9569 auprc 0.4395 f1 0.2892\n",
      "Epoch 08 | train loss 0.8742 auroc 0.8945 auprc 0.1836 f1 0.1618 | val loss 1.0940 auroc 0.8993 auprc 0.2634 f1 0.2008\n",
      "Epoch 09 | train loss 0.7823 auroc 0.9293 auprc 0.2812 f1 0.2281 | val loss 1.0706 auroc 0.8893 auprc 0.2548 f1 0.2258\n",
      "Early stopping.\n",
      "Loading best model and evaluating on TEST…\n",
      "{'auroc': np.float64(0.9589341901715126), 'auprc': np.float64(0.3374902177760662), 'f1': 0.20019572802314697, 'loss': 0.5194872690480653}\n",
      "Saved test_probs.npy, test_true.npy, test_mask.npy\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sepsis hourly sequence‑to‑sequence prediction with LSTM + Attention (PyTorch)\n",
    "-----------------------------------------------------------------------------\n",
    "This script trains a temporal attention model that predicts a binary sepsis\n",
    "label for EACH HOUR of a patient’s ICU stay (seq2seq).\n",
    "\n",
    "It assumes your data are in *long format* like:\n",
    "\n",
    "    Patient_Id | ICULOS | <feature_1> ... <feature_K> | SepsisLabel\n",
    "\n",
    "Key features:\n",
    "- Patient‑wise train/val/test split (no leakage)\n",
    "- Per‑patient sequences with padding and masks\n",
    "- LSTM (bidirectional) + simple temporal attention\n",
    "- Masked loss so padded hours don’t affect training\n",
    "- Metrics: AUROC, AUPRC, F1 at the timestep level (masked)\n",
    "\n",
    "How to use:\n",
    "1) Put a combined CSV at DATA_CSV (one row per patient‑hour), OR\n",
    "   set USE_EXISTING_FULLDATA=True to use an existing `fulldata` DataFrame\n",
    "   already in the notebook environment (same columns).\n",
    "2) Adjust FEATURE_COLS if you want to include/exclude vars.\n",
    "3) Run the script.\n",
    "\n",
    "Notes:\n",
    "- For very imbalanced labels, consider enabling FOCAL_LOSS or POS_WEIGHT.\n",
    "- If your label column is named differently, set LABEL_COL accordingly.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========================\n",
    "# Configuration\n",
    "# ========================\n",
    "DATA_CSV = \"/Users/farhat/Documents/Project/ProcessedData/fullData.csv\"   # Path to long-format CSV (ignored if USE_EXISTING_FULLDATA=True)\n",
    "ID_COL = \"Patient_Id\"\n",
    "TIME_COL = \"ICULOS\"         # integer hours\n",
    "LABEL_COL = \"SepsisLabel\"   # 0/1 per hour\n",
    "USE_EXISTING_FULLDATA = False  # set True if a DataFrame `fulldata` exists in memory\n",
    "\n",
    "# Model & training\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.2\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "PATIENCE = 3                 # early stopping patience\n",
    "\n",
    "# Loss handling for imbalance\n",
    "USE_FOCAL_LOSS = False\n",
    "FOCAL_GAMMA = 2.0\n",
    "POS_WEIGHT = None            # e.g., torch.tensor([5.0]) to up‑weight positives; set after computing prevalence\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ========================\n",
    "# Data loading / preprocessing\n",
    "# ========================\n",
    "\n",
    "def load_fulldata() -> pd.DataFrame:\n",
    "    if USE_EXISTING_FULLDATA and 'fulldata' in globals():\n",
    "        df = globals()['fulldata'].copy()\n",
    "    else:\n",
    "        if not os.path.exists(DATA_CSV):\n",
    "            raise FileNotFoundError(f\"DATA_CSV not found: {DATA_CSV}\\nProvide a combined long‑format CSV or set USE_EXISTING_FULLDATA=True.\")\n",
    "        df = pd.read_csv(DATA_CSV)\n",
    "    # basic checks\n",
    "    for col in [ID_COL, TIME_COL, LABEL_COL]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in data.\")\n",
    "    # sort\n",
    "    df = df.sort_values([ID_COL, TIME_COL]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_feature_cols(df: pd.DataFrame) -> List[str]:\n",
    "    # all non‑id/time/label numeric columns are features\n",
    "    excl = {ID_COL, TIME_COL, LABEL_COL}\n",
    "    feature_cols = [c for c in df.columns if c not in excl]\n",
    "    # keep only numeric\n",
    "    feature_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if not feature_cols:\n",
    "        raise ValueError(\"No numeric feature columns found.\")\n",
    "    return feature_cols\n",
    "\n",
    "\n",
    "def patient_split(df: pd.DataFrame, test_size=0.2, val_size=0.1) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    pids = df[ID_COL].unique()\n",
    "    train_pids, test_pids = train_test_split(pids, test_size=test_size, random_state=SEED, shuffle=True)\n",
    "    # split train into train/val\n",
    "    train_pids, val_pids = train_test_split(train_pids, test_size=val_size, random_state=SEED, shuffle=True)\n",
    "    return train_pids, val_pids, test_pids\n",
    "\n",
    "\n",
    "def per_patient_impute(group: pd.DataFrame, feature_cols: List[str]) -> pd.DataFrame:\n",
    "    # forward‑fill then back‑fill within a patient; then fill remaining with column medians\n",
    "    group = group.copy()\n",
    "    group[feature_cols] = group[feature_cols].ffill().bfill()\n",
    "    return group\n",
    "\n",
    "\n",
    "def compute_feature_stats(df: pd.DataFrame, feature_cols: List[str]) -> Tuple[pd.Series, pd.Series]:\n",
    "    means = df[feature_cols].mean()\n",
    "    stds = df[feature_cols].std(ddof=0).replace(0, 1.0)\n",
    "    return means, stds\n",
    "\n",
    "\n",
    "def normalize_df(df: pd.DataFrame, feature_cols: List[str], means: pd.Series, stds: pd.Series) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[feature_cols] = (df[feature_cols] - means) / stds\n",
    "    df[feature_cols] = df[feature_cols].fillna(0.0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_sequences(df: pd.DataFrame, feature_cols: List[str]) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "    X_list, y_list = [], []\n",
    "    for pid, g in df.groupby(ID_COL, sort=False):\n",
    "        g = g.sort_values(TIME_COL)\n",
    "        X = g[feature_cols].to_numpy(dtype=np.float32)\n",
    "        y = g[LABEL_COL].to_numpy(dtype=np.float32)\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "    return X_list, y_list\n",
    "\n",
    "\n",
    "def pad_and_mask(X_list: List[np.ndarray], y_list: List[np.ndarray]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    max_len = max(len(x) for x in X_list)\n",
    "    feat_dim = X_list[0].shape[1]\n",
    "    N = len(X_list)\n",
    "    X_pad = np.zeros((N, max_len, feat_dim), dtype=np.float32)\n",
    "    y_pad = np.zeros((N, max_len), dtype=np.float32)\n",
    "    mask = np.zeros((N, max_len), dtype=np.float32)  # 1 for valid timesteps\n",
    "    for i, (x, y) in enumerate(zip(X_list, y_list)):\n",
    "        L = len(x)\n",
    "        X_pad[i, :L, :] = x\n",
    "        y_pad[i, :L] = y\n",
    "        mask[i, :L] = 1.0\n",
    "    return torch.tensor(X_pad), torch.tensor(y_pad), torch.tensor(mask)\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Dataset / DataLoader\n",
    "# ========================\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X: torch.Tensor, y: torch.Tensor, mask: torch.Tensor):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.mask = mask\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.mask[idx]\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Model: LSTM + Temporal Attention (per‑timestep preds)\n",
    "# ========================\n",
    "class LSTMAttnSeq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int, bidirectional: bool = True, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_dirs = 2 if bidirectional else 1\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0, bidirectional=bidirectional)\n",
    "        # simple per‑timestep attention over hidden features\n",
    "        self.attn = nn.Linear(self.num_dirs * hidden_dim, 1)\n",
    "        self.proj = nn.Linear(self.num_dirs * hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        h, _ = self.lstm(x)  # (B, T, H*dirs)\n",
    "        w = torch.softmax(self.attn(h), dim=1)  # (B, T, 1) attention over time\n",
    "        context = h * w  # (B, T, H*dirs) — keeps per‑timestep but reweights\n",
    "        logits = self.proj(context).squeeze(-1)  # (B, T)\n",
    "        return logits, w  # return raw logits (use BCEWithLogitsLoss)\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Loss (masked)\n",
    "# ========================\n",
    "class MaskedBCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='none')\n",
    "    def forward(self, logits, targets, mask):\n",
    "        # logits/targets: (B, T); mask: (B, T)\n",
    "        loss = self.loss(logits, targets)\n",
    "        loss = loss * mask\n",
    "        denom = mask.sum().clamp_min(1.0)\n",
    "        return loss.sum() / denom\n",
    "\n",
    "\n",
    "class MaskedFocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    def forward(self, logits, targets, mask):\n",
    "        bce = self.bce(logits, targets)\n",
    "        p = torch.sigmoid(logits)\n",
    "        pt = torch.where(targets == 1, p, 1 - p)\n",
    "        focal = (1 - pt).pow(self.gamma) * bce\n",
    "        focal = focal * mask\n",
    "        denom = mask.sum().clamp_min(1.0)\n",
    "        return focal.sum() / denom\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Metrics (masked)\n",
    "# ========================\n",
    "@torch.no_grad()\n",
    "def masked_metrics(logits, targets, mask) -> dict:\n",
    "    # flatten valid positions only\n",
    "    probs = torch.sigmoid(logits).detach().cpu().numpy().ravel()\n",
    "    y_true = targets.detach().cpu().numpy().ravel()\n",
    "    m = mask.detach().cpu().numpy().ravel().astype(bool)\n",
    "\n",
    "    probs = probs[m]\n",
    "    y_true = y_true[m]\n",
    "\n",
    "    out = {}\n",
    "    if y_true.size == 0 or len(np.unique(y_true)) < 2:\n",
    "        out['auroc'] = np.nan\n",
    "    else:\n",
    "        out['auroc'] = roc_auc_score(y_true, probs)\n",
    "    try:\n",
    "        out['auprc'] = average_precision_score(y_true, probs)\n",
    "    except Exception:\n",
    "        out['auprc'] = np.nan\n",
    "\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    try:\n",
    "        out['f1'] = f1_score(y_true, preds)\n",
    "    except Exception:\n",
    "        out['f1'] = np.nan\n",
    "    return out\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Training / Evaluation loops\n",
    "# ========================\n",
    "\n",
    "def run_epoch(model, loader, criterion, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "    total_loss = 0.0\n",
    "    n_tokens = 0.0\n",
    "    all_logits, all_targets, all_masks = [], [], []\n",
    "\n",
    "    for X, y, mask in loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        logits, _ = model(X)\n",
    "        loss = criterion(logits, y, mask)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * mask.sum().item()\n",
    "        n_tokens += mask.sum().item()\n",
    "        all_logits.append(logits.detach())\n",
    "        all_targets.append(y.detach())\n",
    "        all_masks.append(mask.detach())\n",
    "\n",
    "    epoch_loss = total_loss / max(n_tokens, 1.0)\n",
    "\n",
    "    logits_cat = torch.cat(all_logits, dim=0)\n",
    "    targets_cat = torch.cat(all_targets, dim=0)\n",
    "    masks_cat = torch.cat(all_masks, dim=0)\n",
    "    mets = masked_metrics(logits_cat, targets_cat, masks_cat)\n",
    "    mets['loss'] = epoch_loss\n",
    "    return mets\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Loading data…\")\n",
    "    df = load_fulldata()\n",
    "    feature_cols = get_feature_cols(df)\n",
    "\n",
    "    # Split by patients\n",
    "    tr_pids, va_pids, te_pids = patient_split(df)\n",
    "\n",
    "    # Per‑patient imputation (ffill/bfill)\n",
    "    medians = df[feature_cols].median()\n",
    "    df = df.groupby(ID_COL, group_keys=False).apply(lambda g: per_patient_impute(g, feature_cols))\n",
    "    df[feature_cols] = df[feature_cols].fillna(medians)\n",
    "\n",
    "    # Fit normalization on TRAIN ONLY\n",
    "    train_df = df[df[ID_COL].isin(tr_pids)]\n",
    "    means, stds = compute_feature_stats(train_df, feature_cols)\n",
    "\n",
    "    # Normalize all splits with train stats\n",
    "    df_norm = normalize_df(df, feature_cols, means, stds)\n",
    "\n",
    "    # Build sequences for each split\n",
    "    X_tr_list, y_tr_list = build_sequences(df_norm[df_norm[ID_COL].isin(tr_pids)], feature_cols)\n",
    "    X_va_list, y_va_list = build_sequences(df_norm[df_norm[ID_COL].isin(va_pids)], feature_cols)\n",
    "    X_te_list, y_te_list = build_sequences(df_norm[df_norm[ID_COL].isin(te_pids)], feature_cols)\n",
    "\n",
    "    # Optional: compute pos_weight from TRAIN labels\n",
    "    global POS_WEIGHT\n",
    "    if POS_WEIGHT is None:\n",
    "        pos = sum(y.sum() for y in y_tr_list)\n",
    "        neg = sum(len(y) - y.sum() for y in y_tr_list)\n",
    "        if pos > 0:\n",
    "            POS_WEIGHT = torch.tensor([max(1.0, neg / max(pos, 1.0))], device=device)\n",
    "            print(f\"Using POS_WEIGHT={POS_WEIGHT.item():.3f}\")\n",
    "        else:\n",
    "            POS_WEIGHT = None\n",
    "\n",
    "    # Pad and mask\n",
    "    X_tr, y_tr, m_tr = pad_and_mask(X_tr_list, y_tr_list)\n",
    "    X_va, y_va, m_va = pad_and_mask(X_va_list, y_va_list)\n",
    "    X_te, y_te, m_te = pad_and_mask(X_te_list, y_te_list)\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(SeqDataset(X_tr, y_tr, m_tr), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = DataLoader(SeqDataset(X_va, y_va, m_va), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader  = DataLoader(SeqDataset(X_te, y_te, m_te), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    input_dim = X_tr.shape[2]\n",
    "    model = LSTMAttnSeq2Seq(input_dim, HIDDEN_DIM, NUM_LAYERS, BIDIRECTIONAL, DROPOUT).to(device)\n",
    "\n",
    "    # Loss\n",
    "    if USE_FOCAL_LOSS:\n",
    "        criterion = MaskedFocalLoss(gamma=FOCAL_GAMMA)\n",
    "    else:\n",
    "        criterion = MaskedBCEWithLogitsLoss(pos_weight=POS_WEIGHT)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    best_val = math.inf\n",
    "    patience = PATIENCE\n",
    "\n",
    "    print(\"Training…\")\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_mets = run_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_mets = run_epoch(model, val_loader, criterion, optimizer=None)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | train loss {train_mets['loss']:.4f} auroc {train_mets['auroc']:.4f} auprc {train_mets['auprc']:.4f} f1 {train_mets['f1']:.4f} | \"\n",
    "              f\"val loss {val_mets['loss']:.4f} auroc {val_mets['auroc']:.4f} auprc {val_mets['auprc']:.4f} f1 {val_mets['f1']:.4f}\")\n",
    "\n",
    "        if val_mets['loss'] < best_val - 1e-4:\n",
    "            best_val = val_mets['loss']\n",
    "            patience = PATIENCE\n",
    "            torch.save({'model_state': model.state_dict(), 'config': {\n",
    "                'input_dim': input_dim,\n",
    "                'hidden_dim': HIDDEN_DIM,\n",
    "                'num_layers': NUM_LAYERS,\n",
    "                'bidirectional': BIDIRECTIONAL,\n",
    "                'dropout': DROPOUT\n",
    "            }}, 'best_model.pt')\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    print(\"Loading best model and evaluating on TEST…\")\n",
    "    ckpt = torch.load('best_model.pt', map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state'])\n",
    "\n",
    "    test_mets = run_epoch(model, test_loader, criterion, optimizer=None)\n",
    "    print({k: (None if isinstance(v, float) and (np.isnan(v) or np.isinf(v)) else v) for k, v in test_mets.items()})\n",
    "\n",
    "    # Optionally, save per‑timestep probabilities for test set\n",
    "    all_probs = []\n",
    "    all_true = []\n",
    "    all_masks = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y, mask in test_loader:\n",
    "            X = X.to(device)\n",
    "            logits, _ = model(X)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "            all_true.append(y.numpy())\n",
    "            all_masks.append(mask.numpy())\n",
    "    np.save('test_probs.npy', np.concatenate(all_probs, axis=0))\n",
    "    np.save('test_true.npy',  np.concatenate(all_true, axis=0))\n",
    "    np.save('test_mask.npy',  np.concatenate(all_masks, axis=0))\n",
    "    print(\"Saved test_probs.npy, test_true.npy, test_mask.npy\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
